/**

Genetic algorithm for finding function aproximation. GPU accelerated version

Given data points {x, f(x)+noise} generated by noisy polynomial function
f(x) = c3*x^3 + c2*x^2 + c1*x + c0,
find unknown parameters c1, c2, c3 and c0.


Inputs:
• The set of points on a surface (500–1000);
• The size of population P (1000–2000);
• E_m , D_m – mean and variance for Mutation to generate the random number of mutated genes;
• maxIter - the maximum number of generations, 
  maxConstIter - the maximum number of generations with constant value of the best fitness.

Outputs:
• The time of processing on GPU;
• The set of coefficients of the polynomial that approximates the given set of points;
• The best fitness value;
• The last generation number (number of evaluated iterations).
*/

#include <mpi.h>
#include <iostream>
#include <cstdlib>
#include <cstdio>
#include <cmath>
#include <time.h>
#include <algorithm>
#include <assert.h>     /* assert */

#include "mpi_version_multi.h"

#include "config.h"
#include "nvToolsExt.h"

using namespace std;

// Error handling macros
#define MPI_CHECK(call) \
    if((call) != MPI_SUCCESS) { \
        cerr << "MPI error calling \""#call"\"\n"; \
        my_abort(-1); }

/*
    ---------------------------------------------------------
    |  MPI communication and encapsulated GPU computation   |
    ---------------------------------------------------------
*/
int main(int argc, char **argv)
{
    if(argc != 2) {
        cerr << "Usage: $mpirun -np N ./gpu inputFile" << endl;    
        return -1;
    }

    // Initialize MPI state
    MPI_CHECK(MPI_Init(&argc, &argv));

    // Get our MPI node number and node count
    int commSize, commRank;
    MPI_CHECK(MPI_Comm_size(MPI_COMM_WORLD, &commSize));
    MPI_CHECK(MPI_Comm_rank(MPI_COMM_WORLD, &commRank));
    int deviceID = commRank;

    if(commSize > 4)
    {
        cerr << "Cannot run with more than 4 processes!" << endl;
        return -1;        
    }

    //size of local portion of data
    //TODO population%commSize  != 0
    assert(POPULATION_SIZE % commSize == 0);
    int local_size = POPULATION_SIZE/commSize;    

    //read input data
    //points are the data to approximate by a polynomial
    float *points = readData(argv[1], N_POINTS);
    if(points == NULL)
        return -1;

    /**
        Allocations of GPU memory
        
        Some portion of the code (crossover, mutation, selection)
        is executed only on master process, some workload
        (fitness evaluation) is distributed amongst
        all existing processes.

        -master process allocates data for whole population
        -slave processes only for its own local data part
    */

    //set proper device
    cudaSetDevice(deviceID);
    check_cuda_error("Setting device");

    //device memory for holding input points on all processes
    float *points_dev;
    cudaMalloc(&points_dev, 2*N_POINTS*sizeof(float)); // [x, f(x)+err]
    check_cuda_error("Error allocating device memory");
    cudaMemcpy(points_dev, points, 2*N_POINTS*sizeof(float), cudaMemcpyHostToDevice);
    check_cuda_error("Error copying data");


    //arrays to hold population    
    float *population_dev;
    float *population_dev_local;
    float *population_devT;
    float *population_dev_localT;
    float *newPopulation_dev;
    if(commRank == 0){
        cudaMalloc(&population_dev, POPULATION_SIZE * INDIVIDUAL_LEN * sizeof(float));
        check_cuda_error("Error allocating device memory");

        cudaMalloc(&population_devT, POPULATION_SIZE * INDIVIDUAL_LEN * sizeof(float));
        check_cuda_error("Error allocating device memory");

        cudaMalloc(&newPopulation_dev, POPULATION_SIZE * INDIVIDUAL_LEN * sizeof(float));
        check_cuda_error("Error allocating device memory");
    }

    cudaMalloc(&population_dev_local, local_size * INDIVIDUAL_LEN * sizeof(float));
    check_cuda_error("Error allocating device memory"); 

    cudaMalloc(&population_dev_localT, local_size * INDIVIDUAL_LEN * sizeof(float));
    check_cuda_error("Error allocating device memory"); 

    //arrays that keeps fitness of individuals withing current population
    float *fitness_dev;    
    if(commRank == 0){
        cudaMalloc(&fitness_dev, POPULATION_SIZE*sizeof(float));
        check_cuda_error("Error allocating device memory");
    }else{
        cudaMalloc(&fitness_dev, local_size * sizeof(float));
        check_cuda_error("Error allocating device memory");   
    }

    //curand random states
    curandState *state_random;
    if(commRank == 0){
        cudaMalloc((void **)&state_random,POPULATION_SIZE * sizeof(curandState));
        check_cuda_error("Allocating memory for curandState");
    }

    //mutation probabilities, mutation done only locally
    float* mutIndivid_d;
    if(commRank == 0){
        cudaMalloc((void **) &mutIndivid_d, POPULATION_SIZE * sizeof(float));
        check_cuda_error("Allocating memory in mutIndivid_d");
    }

    float* mutGene_d;
    if(commRank == 0){
        cudaMalloc((void **)&mutGene_d, POPULATION_SIZE * INDIVIDUAL_LEN*sizeof(float));
        check_cuda_error("Allocating memory in mutGene_d");
    }

    //create PRNG for generating mutation probabilities
    curandGenerator_t generator;

    if(commRank == 0){
        curandCreateGenerator(&generator, CURAND_RNG_PSEUDO_DEFAULT);
        check_cuda_error("Error in curandCreateGenerator");

        curandSetPseudoRandomGeneratorSeed(generator, 0);
        check_cuda_error("Error in curandSeed");
    }
    
    //key value for sorting, sorting done only on master process
    int *indexes_dev;
    thrust::device_ptr<int> indexes_thrust;
    thrust::device_ptr<float> fitnesses_thrust;
    if(commRank == 0){
        cudaMalloc(&indexes_dev, POPULATION_SIZE*sizeof(int));
        check_cuda_error("Error allocating device memory");

        //recast device pointers into thrust copatible pointers
        indexes_thrust = thrust::device_pointer_cast(indexes_dev);
        fitnesses_thrust = thrust::device_pointer_cast(fitness_dev);
    }



    /**
        Main GA loop
    */

    //Initialize first population (with zeros or some random values) and curandState*
    if(commRank == 0)
        doInitPopulation(population_dev, state_random);
    
    int t1 = clock(); //start timer

    int generationNumber = 0;
    int noChangeIter = 0;

    float bestFitness = INFINITY;
    float previousBestFitness = INFINITY;

	while ( (generationNumber < maxGenerationNumber)
            /*&& (bestFitness > targetErr)
            && (noChangeIter < maxConstIter)*/ )
	{
		generationNumber++;
	
        /** Crossover first half of the population and create new population 
        *   and afterward mutate new population
        */
        if(commRank == 0)
        {
    		doCrossover(population_dev, state_random);

		    doMutation(population_dev, state_random, POPULATION_SIZE,
                       mutIndivid_d, mutGene_d, generator);

        }


        /** Distribute population to all processes to perform fitness evaluation
            Population is stored in transposed matrix to access global memory in coalesced
            pattern - first POPULATION_SIZE entries are first aleles of population,
            next POPULATION_SIZE entries are second aleles and so on...
            
            Before we scatter data we need to transpose the population matrix,
            so that we scatter whole individuals and use only single MPI_Scatter call  */

        //transpose population matrix so it can be scattered in one scatter call
        if(commRank == 0)
            doTranspose(population_devT, population_dev, POPULATION_SIZE);

        nvtxRangePushA("Scatter");
            MPI_CHECK(
                MPI_Scatter(
                    population_devT, INDIVIDUAL_LEN*local_size, MPI_FLOAT,
                    population_dev_localT, INDIVIDUAL_LEN*local_size, MPI_FLOAT,
                    0, MPI_COMM_WORLD)
            );
        nvtxRangePop();

        //transpose recieved population chunk, so it can be accessed in coalesced way
        doTranspose_inverse(population_dev_local, population_dev_localT, local_size);

        /** evaluate fitness of individuals in local portion of population */
		doFitness_evaluate(population_dev_local, points_dev, fitness_dev, local_size);

        nvtxRangePushA("Gather 2");
        MPI_CHECK(
            MPI_Gather(fitness_dev, local_size, MPI_FLOAT,
                       fitness_dev, local_size, MPI_FLOAT,
                       0, MPI_COMM_WORLD)
        );
        nvtxRangePop();


        /** select individuals for mating to create the next generation,
            i.e. sort population according to its fitness and keep
            fittest individuals first in population  */
        if(commRank == 0) {

            nvtxRangePushA("Selection wrapper");
            doSelection(fitnesses_thrust, indexes_thrust, indexes_dev,
                        population_dev, newPopulation_dev);    
            nvtxRangePop();

            //swap populations        
            float *tmp = population_dev;
            population_dev = newPopulation_dev;
            newPopulation_dev = tmp;

            /** time step evaluation - convergence criterion check */
            //get BEST FITNESS to host
            cudaMemcpy(&bestFitness, fitness_dev, sizeof(float), cudaMemcpyDeviceToHost);
            check_cuda_error("Coping fitnesses_dev[0] to host");
            
            //check if the fitness is decreasing or if we are stuck at local minima
            if(fabs(bestFitness - previousBestFitness) < 0.01f)
                noChangeIter++;
            else
                noChangeIter = 0;
            previousBestFitness = bestFitness;

            //log message
            #if defined(DEBUG)
            cout << "#" << generationNumber<< " Fitness: " << bestFitness << \
            " Iterations without change: " << noChangeIter << endl;
            #endif
        }

	}

    int t2 = clock(); //stop timer


    /**
       Process results on master process.
    */
    int masterProcess = 0;

    if(commRank == masterProcess)
    {
  
        cout << "------------------------------------------------------------" << endl;    
        cout << "Finished! Found Solution: " << endl;       

        //solution with the best params of a polynomial 
        for(int i=0; i<INDIVIDUAL_LEN; i++){ 
            float solution;            
            
            //get best individual
            cudaMemcpy(&solution, &population_dev[i*POPULATION_SIZE],
                       sizeof(float), cudaMemcpyDeviceToHost);
            check_cuda_error("Coping fitnesses_dev[0] to host");
        
            //print
            cout << "\tc" << i << " = " << solution << endl;
        }

        cout << "Best fitness: " << bestFitness << endl \
        << "Generations: " << generationNumber << endl;

        cout << "Time for GPU calculation + communication equals \033[35m" \
            << (t2-t1)/(double)CLOCKS_PER_SEC << " seconds\033[0m" << endl;

    }

    
    /**
        Free memory
    */
    cudaFree(points_dev);//input points
    check_cuda_error("points free");
    
    cudaFree(fitness_dev);//fitness array
    check_cuda_error("fitness free");

    if(commRank == 0){
        cudaFree(indexes_dev);//key for sorting
        check_cuda_error("indexes free");

        cudaFree(population_dev);
        check_cuda_error("population free");

        cudaFree(population_devT);
        check_cuda_error("population free");
    }
   
    cudaFree(population_dev_local);
    check_cuda_error("population local free");

    cudaFree(population_dev_localT);
    check_cuda_error("population local free");

    if(commRank == 0){
        cudaFree(newPopulation_dev);
        check_cuda_error("newPopulation free");
  
        cudaFree(state_random);//state curand
        check_cuda_error("state free");

        cudaFree(mutIndivid_d);//mutation probability
        check_cuda_error("mutInd free");

        cudaFree(mutGene_d);//mutation probability
        check_cuda_error("mutGene free");

        curandDestroyGenerator(generator);
        check_cuda_error("Destroying generator");
    }

    MPI_CHECK(MPI_Finalize());
    
    return 0;
}

//------------------------------------------------------------------------------

float *readData(const char *name, const int POINTS_CNT)
{
    FILE *file = fopen(name, "r");
 
	float *points = new float[2 * POINTS_CNT]; 
    if (file)
    {
        //x, f(x)
        for (int k = 0; k < POINTS_CNT; k++)
        {
        	if (fscanf(file, "%f %f", &points[k], &points[POINTS_CNT + k]) == EOF)
        	{
        		cerr << "Unexpected end of input data" << endl;
        		exit(1);
        	}
		}
        fclose(file);
        cout << "Reading file - success!" << endl;
    }
    else
    {
        cerr << "Error while opening the file " << name << "!!!" << endl;
        delete [] points;
        exit(1);
    }

    return points;
}

// Shut down MPI cleanly if something goes wrong
void my_abort(int err)
{
    cout << "Test FAILED\n";
    MPI_Abort(MPI_COMM_WORLD, err);
}

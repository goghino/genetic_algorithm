/**

Genetic algorithm for finding function aproximation. GPU accelerated version

Given data points {x, f(x)+noise} generated by noisy polynomial function
f(x) = c3*x^3 + c2*x^2 + c1*x + c0,
find unknown parameters c1, c2, c3 and c0.


Inputs:
• The set of points on a surface (500–1000);
• The size of population P (1000–2000);
• E_m , D_m – mean and variance for Mutation to generate the random number of mutated genes;
• maxIter - the maximum number of generations, 
  maxConstIter - the maximum number of generations with constant value of the best fitness.

Outputs:
• The time of processing on GPU;
• The set of coefficients of the polynomial that approximates the given set of points;
• The best fitness value;
• The last generation number (number of evaluated iterations).
*/

#include <mpi.h>
#include <iostream>
#include <cstdlib>
#include <cstdio>
#include <cmath>
#include <time.h>
#include <algorithm>
#include <assert.h>     /* assert */

#include <iterator>
#include <fstream>
#include <vector>

#include "mpi_version_multi.h"

#include "config.h"
#include "nvToolsExt.h"

using namespace std;

// Reads input file with noisy points. Points will be approximated by 
// polynomial function using GA.
static float *readData(const char *name, int *N_POINTS);

// Error handling macros
#define MPI_CHECK(call) \
    if((call) != MPI_SUCCESS) { \
        cerr << "MPI error calling \""#call"\"\n"; \
        my_abort(-1); }

/*
    ---------------------------------------------------------
    |  MPI communication and encapsulated GPU computation   |
    ---------------------------------------------------------
*/
int main(int argc, char **argv)
{
    if(argc != 2) {
        cerr << "Usage: $mpirun -np N ./gpu inputFile" << endl;    
        return -1;
    }

    // Initialize MPI state
    MPI_CHECK(MPI_Init(&argc, &argv));

    // Get our MPI node number and node count
    int commSize, commRank;
    MPI_CHECK(MPI_Comm_size(MPI_COMM_WORLD, &commSize));
    MPI_CHECK(MPI_Comm_rank(MPI_COMM_WORLD, &commRank));
    int deviceID = commRank;

    if(commSize > 4)
    {
        cerr << "Cannot run with more than 4 processes!" << endl;
        return -1;        
    }

    //size of local portion of data
    //TODO population%commSize  != 0
    assert(POPULATION_SIZE % commSize == 0);
    int local_size = POPULATION_SIZE/commSize;    

    //read input data - points to approximate by a polynomial
    int N_POINTS;
    float *points = readData(argv[1], &N_POINTS);
    if(points == NULL)
        return -1;

    /**
        Allocations of GPU memory
        
        Some portion of the code (crossover, mutation, selection)
        is executed only on master process, some workload
        (fitness evaluation) is distributed amongst
        all existing processes.

        -master process allocates data for whole population
        -slave processes only for its own local data part
    */

    //set proper device
    cudaSetDevice(deviceID);
    check_cuda_error("Setting device");

    //device memory for holding input points on all processes
    float *points_dev;
    cudaMalloc(&points_dev, 2*N_POINTS*sizeof(float)); // [x, f(x)+err]
    check_cuda_error("Error allocating device memory");
    cudaMemcpy(points_dev, points, 2*N_POINTS*sizeof(float), cudaMemcpyHostToDevice);
    check_cuda_error("Error copying data");


    //arrays to hold population    
    float *population_dev;
    float *population_dev_local;
    float *population_devT;
    float *population_dev_localT;
    float *newPopulation_dev;
    if(commRank == 0){
        cudaMalloc(&population_dev, POPULATION_SIZE * INDIVIDUAL_LEN * sizeof(float));
        check_cuda_error("Error allocating device memory");

        cudaMalloc(&population_devT, POPULATION_SIZE * INDIVIDUAL_LEN * sizeof(float));
        check_cuda_error("Error allocating device memory");

        cudaMalloc(&newPopulation_dev, POPULATION_SIZE * INDIVIDUAL_LEN * sizeof(float));
        check_cuda_error("Error allocating device memory");
    }

    cudaMalloc(&population_dev_local, local_size * INDIVIDUAL_LEN * sizeof(float));
    check_cuda_error("Error allocating device memory"); 

    cudaMalloc(&population_dev_localT, local_size * INDIVIDUAL_LEN * sizeof(float));
    check_cuda_error("Error allocating device memory"); 

    //arrays that keeps fitness of individuals withing current population
    float *fitness_dev;    
    if(commRank == 0){
        cudaMalloc(&fitness_dev, POPULATION_SIZE*sizeof(float));
        check_cuda_error("Error allocating device memory");
    }else{
        cudaMalloc(&fitness_dev, local_size * sizeof(float));
        check_cuda_error("Error allocating device memory");   
    }

    //curand random states
    curandState *state_random;
    if(commRank == 0){
        cudaMalloc((void **)&state_random,POPULATION_SIZE * sizeof(curandState));
        check_cuda_error("Allocating memory for curandState");
    }

    //mutation probabilities, mutation done only locally
    float* mutIndivid_d;
    if(commRank == 0){
        cudaMalloc((void **) &mutIndivid_d, POPULATION_SIZE * sizeof(float));
        check_cuda_error("Allocating memory in mutIndivid_d");
    }

    float* mutGene_d;
    if(commRank == 0){
        cudaMalloc((void **)&mutGene_d, POPULATION_SIZE * INDIVIDUAL_LEN*sizeof(float));
        check_cuda_error("Allocating memory in mutGene_d");
    }

    //create PRNG for generating mutation probabilities
    curandGenerator_t generator;

    if(commRank == 0){
        curandCreateGenerator(&generator, CURAND_RNG_PSEUDO_DEFAULT);
        check_cuda_error("Error in curandCreateGenerator");

        curandSetPseudoRandomGeneratorSeed(generator, 0);
        check_cuda_error("Error in curandSeed");
    }
    
    //key value for sorting, sorting done only on master process
    int *indexes_dev;
    thrust::device_ptr<int> indexes_thrust;
    thrust::device_ptr<float> fitnesses_thrust;
    if(commRank == 0){
        cudaMalloc(&indexes_dev, POPULATION_SIZE*sizeof(int));
        check_cuda_error("Error allocating device memory");

        //recast device pointers into thrust copatible pointers
        indexes_thrust = thrust::device_pointer_cast(indexes_dev);
        fitnesses_thrust = thrust::device_pointer_cast(fitness_dev);
    }



    /**
        Main GA loop
    */

    //Initialize first population (with zeros or some random values) and curandState*
    if(commRank == 0)
        doInitPopulation(population_dev, state_random);
    
    int t1 = clock(); //start timer

    int generationNumber = 0;
    int noChangeIter = 0;

    float bestFitness = INFINITY;
    float previousBestFitness = INFINITY;

#ifdef PERF_METRIC
float *t_fitness;
cudaEvent_t start, stop;
const int TOTAL_POINTS = N_POINTS*POPULATION_SIZE;
if(commRank == 0){    
    t_fitness = new float[maxGenerationNumber];
    
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
}
#endif

	while ( (generationNumber < maxGenerationNumber)
            /*&& (bestFitness > targetErr)
            && (noChangeIter < maxConstIter)*/ )
	{
		generationNumber++;
	
        /** Crossover first half of the population and create new population 
        *   and afterward mutate new population
        */
        if(commRank == 0)
        {
    		doCrossover(population_dev, state_random);

		    doMutation(population_dev, state_random, POPULATION_SIZE,
                       mutIndivid_d, mutGene_d, generator);

        }


        /** Distribute population to all processes to perform fitness evaluation
            Population is stored in transposed matrix to access global memory in coalesced
            pattern - first POPULATION_SIZE entries are first aleles of population,
            next POPULATION_SIZE entries are second aleles and so on...
            
            Before we scatter data we need to transpose the population matrix,
            so that we scatter whole individuals and use only single MPI_Scatter call  */

#ifdef PERF_METRIC		
    if(commRank == 0){
        cudaEventRecord(start, 0);
    }
#endif

        //transpose population matrix so it can be scattered in one scatter call
        if(commRank == 0)
            doTranspose(population_devT, population_dev, POPULATION_SIZE);

#ifdef DEBUG
        nvtxRangePushA("Scatter");
#endif
            MPI_CHECK(
                MPI_Scatter(
                    population_devT, INDIVIDUAL_LEN*local_size, MPI_FLOAT,
                    population_dev_localT, INDIVIDUAL_LEN*local_size, MPI_FLOAT,
                    0, MPI_COMM_WORLD)
            );
#ifdef DEBUG
        nvtxRangePop();
#endif
        //transpose recieved population chunk, so it can be accessed in coalesced way
        doTranspose_inverse(population_dev_local, population_dev_localT, local_size);

        /** evaluate fitness of individuals in local portion of population */
		doFitness_evaluate(population_dev_local, points_dev, N_POINTS, fitness_dev, local_size);

#ifdef DEBUG
        nvtxRangePushA("Gather 2");
#endif        
        MPI_CHECK(
            MPI_Gather(fitness_dev, local_size, MPI_FLOAT,
                       fitness_dev, local_size, MPI_FLOAT,
                       0, MPI_COMM_WORLD)
        );
#ifdef DEBUG
        nvtxRangePop();
#endif

#ifdef PERF_METRIC
    if(commRank == 0){
        cudaEventRecord(stop, 0);
        cudaEventSynchronize(stop);
        cudaEventElapsedTime(&(t_fitness[generationNumber-1]), start, stop);
        
        //total points in generation / kernel execution time 
        t_fitness[generationNumber-1] = TOTAL_POINTS/t_fitness[generationNumber-1]/1000000;     
    }
#endif

        /** select individuals for mating to create the next generation,
            i.e. sort population according to its fitness and keep
            fittest individuals first in population  */
        if(commRank == 0) {
#ifdef DEBUG
            nvtxRangePushA("Selection wrapper");
#endif
            doSelection(fitnesses_thrust, indexes_thrust, indexes_dev,
                        population_dev, newPopulation_dev);    
#ifdef DEBUG 
           nvtxRangePop();
#endif

            //swap populations        
            float *tmp = population_dev;
            population_dev = newPopulation_dev;
            newPopulation_dev = tmp;

            /** time step evaluation - convergence criterion check */
            //get BEST FITNESS to host
            cudaMemcpy(&bestFitness, fitness_dev, sizeof(float), cudaMemcpyDeviceToHost);
            check_cuda_error("Coping fitnesses_dev[0] to host");
            
            //check if the fitness is decreasing or if we are stuck at local minima
            if(fabs(bestFitness - previousBestFitness) < 0.01f)
                noChangeIter++;
            else
                noChangeIter = 0;
            previousBestFitness = bestFitness;

            //log message
            #ifdef DEBUG
            cout << "#" << generationNumber<< " Fitness: " << bestFitness << \
            " Iterations without change: " << noChangeIter << endl;
            #endif
        }

	}

    int t2 = clock(); //stop timer


    /**
       Process results on master process.
    */
    int masterProcess = 0;

    if(commRank == masterProcess)
    {
  
        cout << "------------------------------------------------------------" << endl;    
        cout << "Finished! Found Solution: " << endl;       

        //solution with the best params of a polynomial 
        for(int i=0; i<INDIVIDUAL_LEN; i++){ 
            float solution;            
            
            //get best individual
            cudaMemcpy(&solution, &population_dev[i*POPULATION_SIZE],
                       sizeof(float), cudaMemcpyDeviceToHost);
            check_cuda_error("Coping fitnesses_dev[0] to host");
        
            //print
            cout << "\tc" << i << " = " << solution << endl;
        }

        cout << "Best fitness: " << bestFitness << endl \
        << "Generations: " << generationNumber << endl;

        cout << "Time for GPU calculation + communication equals \033[35m" \
            << (t2-t1)/(double)CLOCKS_PER_SEC << " seconds\033[0m" << endl;

#ifdef PERF_METRIC
        //process performance numbers, mean and stddev
        double sum = 0;
        for (int i = 0; i < generationNumber; i++)
        {
            sum = sum + t_fitness[i];
        }
        double average = sum / generationNumber;

        sum = 0;
        for (int i = 0; i < generationNumber; i++)
        {
            sum = sum + pow((t_fitness[i] - average), 2);
        }
        double variance = sum / generationNumber;
        double std_deviation = sqrt(variance);

        cout << "Performance: " << average << " GPoints/s with stddev " << std_deviation << endl;
#endif

    }

    
    /**
        Free memory
    */
    cudaFree(points_dev);//input points
    check_cuda_error("points free");
    
    cudaFree(fitness_dev);//fitness array
    check_cuda_error("fitness free");

    if(commRank == 0){
        cudaFree(indexes_dev);//key for sorting
        check_cuda_error("indexes free");

        cudaFree(population_dev);
        check_cuda_error("population free");

        cudaFree(population_devT);
        check_cuda_error("population free");
    }
   
    cudaFree(population_dev_local);
    check_cuda_error("population local free");

    cudaFree(population_dev_localT);
    check_cuda_error("population local free");

    if(commRank == 0){
        cudaFree(newPopulation_dev);
        check_cuda_error("newPopulation free");
  
        cudaFree(state_random);//state curand
        check_cuda_error("state free");

        cudaFree(mutIndivid_d);//mutation probability
        check_cuda_error("mutInd free");

        cudaFree(mutGene_d);//mutation probability
        check_cuda_error("mutGene free");

        curandDestroyGenerator(generator);
        check_cuda_error("Destroying generator");

#ifdef PERF_METRIC
    delete [] t_fitness; 
    cudaEventDestroy(start);
    cudaEventDestroy(stop);        
#endif
    }

    MPI_CHECK(MPI_Finalize());
    
    return 0;
}

//------------------------------------------------------------------------------

static float *readData(const char *file_name, int *N_POINTS)
{
    std::ifstream is(file_name);
    if(!is.is_open())
    {
      cerr << "Error opening file " << file_name << endl;
      exit(1);
    }

    std::istream_iterator<double> start(is), end;
    std::vector<double> points(start, end);

    cout << "Reading file - success!" << endl;

    *N_POINTS = points.size()/2;

    float *points_arr = new float[points.size()];

    //rearrange points array so that first half contains x values, the other f(x)
    int i = 0;
    int N = points.size()/2;
    for (std::vector<double>::iterator it = points.begin() ; it != points.end(); ++it)
    {
        if (i % 2 == 0)
            points_arr[i/2] = *it;
        else
            points_arr[i/2 + N] = *it;

        i++;
    }

    return points_arr;
}

// Shut down MPI cleanly if something goes wrong
void my_abort(int err)
{
    cout << "Test FAILED\n";
    MPI_Abort(MPI_COMM_WORLD, err);
}
